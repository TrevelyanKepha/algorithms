{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a data structure is a systematic way of organizing and accessing data\n",
    "- an algorithm is a step-by-step procedure for performing some task in a finite amount of time\n",
    "- The primary analysis tool we will use involves characterizing the running times of algorithms and data structure operations, with space usage also being of interest.\n",
    "- In general, the running time of an algorithm or data structure operation increases with the input size, although it may also   vary for different inputs of the same size.\n",
    "- We are interested in characterizing an algorithm’s running time as a function of the input size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If an algorithm has been implemented, we can study its running time by executing it on various test inputs and recording the time spent during each execution.\n",
    "- A simple approach for doing this in Python is by using the time function of the time module. \n",
    "- This function reports the number of seconds, or fractions thereof\n",
    "eg:\n",
    "from time import time\n",
    "start time = time( )                  # record the starting time\n",
    "run algorithm\n",
    "end time = time( )                    # record the ending time\n",
    "elapsed = end time − start time       # compute the elapsed time\n",
    "\n",
    "*There are three major limitations to their use for algorithm analysis:*\n",
    "- Experimental running times of two algorithms are difficult to directly compare unless the experiments are performed in the same hardware and software environments.\n",
    "- Experiments can be done only on a limited set of test inputs; hence, they leave out the running times of inputs not included in the experiment (and these inputs may be important).\n",
    "- An algorithm must be fully implemented in order to execute it to study its running time experimentally.\n",
    "\n",
    "This last requirement is the most serious drawback to the use of experimental studies. At early stages of design, when considering a choice of data structures or algorithms, it would be foolish to spend a significant amount of time implementing an approach that could easily be deemed inferior by a higher-level analysis.\n",
    "\n",
    "*Our goal is to develop an approach to analyzing the efficiency of algorithms that:*\n",
    "1. Allows us to evaluate the relative efficiency of any two algorithms in a way that is independent of the hardware and software environment.\n",
    "2. Is performed by studying a high-level description of the algorithm without need for implementation.\n",
    "3. Takes into account all possible inputs.\n",
    "\n",
    "To analyze the running time of an algorithm without performing experiments, we perform an analysis directly on a high-level description of the algorithm (either in the form of an actual code fragment, or language-independent pseudo-code). \n",
    "We define a set of primitive operations such as the following:\n",
    "- Assigning an identifier to an object\n",
    "- Determining the object associated with an identifier\n",
    "- Performing an arithmetic operation (for example, adding two numbers)\n",
    "- Comparing two numbers\n",
    "- Accessing a single element of a Python list by index\n",
    "- Calling a function (excluding operations executed within the function)\n",
    "- Returning from a function.\n",
    "\n",
    "Instead of trying to determine the specific execution time of each primitive operation, we will simply count how many primitive operations are executed, and use this number t as a measure of the running time of the algorithm.\n",
    "\n",
    "*Measuring Operations as a Function of Input Size*\n",
    "\n",
    "To capture the order of growth of an algorithm’s running time, we will associate, with each algorithm, a function f(n) that characterizes the number of primitive operations that are performed as a function of the input size n\n",
    "\n",
    "*Focusing on the Worst-Case Input*\n",
    "\n",
    "An algorithm may run faster on some inputs than it does on others of the same size.\n",
    "Thus, we may wish to express the running time of an algorithm as the function of the input size obtained by taking the average over all possible inputs of the same size.\n",
    "Worst-case analysis is much easier than average-case analysis, as it requires only the ability to identify the worst-case input, which is often simple.\n",
    "This approach typically leads to better algorithms. \n",
    "Making the standard of success for an algorithm to perform well in the worst case necessarily requires that it will do well\n",
    "on every input. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Functions Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Constant Function\n",
    "The simplest function we can think of is the constant function. \n",
    "This is the function,\n",
    "\n",
    " **f(n) = c,**   \n",
    " \n",
    "for some fixed constant c.\n",
    "\n",
    "for any argument n, the constant function f(n) assigns the value c. It does not matter what the value of n is; f(n) will always be equal to the constant value c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Logarithm Function\n",
    "One of the interesting and sometimes even surprising aspects of the analysis of data structures and algorithms is the ubiquitous presence of the logarithm function,\n",
    "\n",
    "**f(n) = logb n**, for some constant b > 1. \n",
    "\n",
    "This function is defined as follows:\n",
    "x = logb n if and only if bx = n.\n",
    "By definition, logb 1 = 0. The value b is known as the base of the logarithm\n",
    "\n",
    "The most common base for the logarithm function in computer science is 2, as computers store integers in binary, and because a common operation in many algorithms is to repeatedly divide an input in half. \n",
    "In fact, this base is so common that we will typically omit it from the notation when it is 2.\n",
    "logn = log2 n\n",
    "\n",
    "In particular, we can easily compute the smallest integer greater than or equal to logb n (its so-called ceiling, logb n\u000e). \n",
    "For positive integer, n, this value is equal to the number of times we can divide n by b before we get a number less than or equal to 1. \n",
    "For example, the evaluation of log3 27\u000e is 3, because ((27/3)/3)/3 = 1.\n",
    "\n",
    "Given real numbers a > 0, b > 1, c > 0 and d > 1, we have:\n",
    "1. logb(ac) = logb a+logb c\n",
    "2. logb(a/c) = logb a−logb c\n",
    "3. logb(a**c) = clogb a   \n",
    "4. logb a = logd a/logd b\n",
    "5. blogd a = alogd b\n",
    "\n",
    "hence\n",
    "\n",
    "- log(2n) = log2+logn = 1+logn, by rule 1\n",
    "- log(n/2) = logn−log2 = logn−1, by rule 2\n",
    "- log n3 = 3log n, by rule 3\n",
    "- log 2n = nlog 2 = n · 1 = n, by rule 3\n",
    "- log4 n = (logn)/log 4 = (logn)/2, by rule 4\n",
    "- 2log n = nlog 2 = n1 = n, by rule 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Linear Function\n",
    "\n",
    "**f(n) = n.**\n",
    "\n",
    "\n",
    "That is, given an input value n, the linear function f assigns the value n itself.\n",
    "This function arises in algorithm analysis any time we have to do a single basic operation for each of n elements\n",
    "\n",
    "The linear function also represents the best running time we can hope to achieve for any algorithm that processes each of n objects that are not already in the computer’s memory, because reading in the n objects already requires n operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The N-Log-N Function\n",
    "\n",
    "**f(n) = nlogn**     - the function that assigns to an input n the value of n times the logarithm base-two of n\n",
    "\n",
    "This function grows a little more rapidly than the linear function and a lot less rapidly than the quadratic function; therefore, we would greatly prefer an algorithm with a running time that is proportional to nlogn, than one with quadratic\n",
    "running time.\n",
    "\n",
    "For example, the fastest possible algorithms for sorting n arbitrary values require time proportional to nlog n.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Quadratic Function\n",
    "f(n) = n**2        -given an input value n, the function f assigns the product “n squared”).\n",
    "\n",
    "The main reason why the quadratic function appears in the analysis of algorithms is that there are many algorithms that have nested loops, where the inner loop performs a linear number of operations and the outer loop is performed a linear number of times. \n",
    "Thus, in such cases, the algorithm performs n · n = n2 operations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Cubic Function and Other Polynomials\n",
    "f(n) = n**3\n",
    "\n",
    "which assigns to an input value n the product of n with itself three times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Exponential Function\n",
    "f(n) = b**n\n",
    "\n",
    "where b is a positive constant, called the base, and the argument n is the exponent.\n",
    "\n",
    "function f(n) assigns to the input argument n the value obtained by multiplying the base b by itself n times.\n",
    "(Exponent Rules): Given positive integers a, b, and c, we have\n",
    "1. (b**a)**c = b**(ac)\n",
    "2. b**ab**c = b**(a+c)\n",
    "3. b**a/b**c = b**(a−c)\n",
    "\n",
    "For example, we have the following:\n",
    "• 256 = 162 = (24)2 = 2**(4·2) = 2**8 = 256 (Exponent Rule 1)\n",
    "• 243 = 35 = 32+3 = 3**23**3 = 9 · 27 = 243 (Exponent Rule 2)\n",
    "• 16 = 1024/64 = 2**10/2**6 = 2**(10−6) = 2**4 = 16 (Exponent Rule 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Sums\n",
    "Suppose we have a loop for which each iteration takes a multiplicative factor longer\n",
    "than the previous one\n",
    "\n",
    "∑i=0 ai = 1+a+a2 +···+an\n",
    "(remembering that a0 = 1 if a > 0). This summation is equal to\n",
    "(an+1 −1)/a−1 .\n",
    "These summations are called geometric summations, because each term is geometrically larger than the previous one if a > 1. For example,\n",
    "\n",
    "1+2+4+8+···+2**(n−1) = 2**(n −1),\n",
    "for this is the largest integer that can be represented in binary notation using n bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asymptotic Analysis\n",
    "In algorithm analysis, we focus on the growth rate of the running time as a function of the input size n, taking a “big-picture” approach.\n",
    "\n",
    "def find max(data):\n",
    "”””Return the maximum element from a nonempty Python list.”””\n",
    "  biggest = data[0]          # The initial value to beat\n",
    "  for val in data:           # For each value:\n",
    "  if val > biggest           # if it is greater than the best so far,\n",
    "  biggest = val              # we have found a new best (so far)\n",
    "  return biggest             # When loop ends, biggest is the max\n",
    "  \n",
    "This is a classic example of an algorithm with a running time that grows proportional to n, as the loop executes once for each data element, with some fixed number of primitive operations executing for each pass.\n",
    "\n",
    "\n",
    "### The “Big-Oh” Notation\n",
    "Let f(n) and g(n) be functions mapping positive integers to positive real numbers.\n",
    "We say that **f(n) is O(g(n))** if there is a real constant c > 0 and an integer constant n0 ≥ 1 such that\n",
    "**f(n) ≤ cg(n), for n ≥ n0.\n",
    "\n",
    "This definition is often referred to as the “big-Oh” notation, for it is sometimes pronounced as “ f(n) is big-Oh of g(n).”\n",
    "\n",
    "**The big-Oh notation allows us to say that a function f(n) is “less than or equal to” another function g(n) up to a constant factor and in the asymptotic sense as n grows toward infinity.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
